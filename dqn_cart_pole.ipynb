{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37fbb079-7f3e-4624-9990-0a486c1ab6b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/rl/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from rl_toolkit.mlp import MLP\n",
    "from rl_toolkit.models import DQN\n",
    "from rl_toolkit.experience_replay import SARSReplayBuffer\n",
    "from rl_toolkit.agents import DQNAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94f4190f-7a24-445e-b0e3-de186d3bd336",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b58645-8c8c-4af2-b7f8-e6fcdd88dd15",
   "metadata": {},
   "source": [
    "## Initialize replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c01dcdd-2e84-493e-a68a-d69b08ad8330",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = SARSReplayBuffer(200)\n",
    "observation, _ = env.reset(return_info=True)\n",
    "for _ in range(200):\n",
    "    action = env.action_space.sample()\n",
    "    new_observation, reward, done, info = env.step(action)\n",
    "    sars = (observation, action, reward - done, new_observation)\n",
    "    replay_buffer.add(sars)\n",
    "    if done:\n",
    "        new_observation, info = env.reset(return_info=True)\n",
    "    observation = new_observation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48e5888-fa0c-437b-aa7b-46ec3f8f21a7",
   "metadata": {},
   "source": [
    "# Train & Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b803d254-2ae5-42cb-8bac-a61f48ff813e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "\n",
    "class DQNTrainer:\n",
    "    def __init__(self, dqn_agent: DQNAgent):\n",
    "        self.dqn_agent = dqn_agent\n",
    "        self.gamma = 0.98\n",
    "        self.optimizer = Adam(dqn_agent.backbone.parameters(), lr=1e-5)\n",
    "        \n",
    "    def train(self, batch: list):\n",
    "        batch = list(map(torch.tensor, batch))\n",
    "        states, actions, rewards, new_states = batch\n",
    "        state_q = self.dqn_agent.backbone(torch.tensor(states))\n",
    "        action_q = state_q[list(range(len(state_q))), actions]\n",
    "        with torch.no_grad():\n",
    "            new_state_q = self.dqn_agent.backbone(torch.tensor(new_states))\n",
    "            new_state_best_action_q = new_state_q.max(axis=-1).values\n",
    "            target_q = rewards + self.gamma * new_state_best_action_q\n",
    "            target_q[~rewards.bool()] = 0\n",
    "        loss = torch.mean((target_q - action_q) ** 2, axis=0)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "191d9249-7440-4258-abc2-69bcff8b56f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    observation, _ = env.reset(return_info=True)\n",
    "    losses = []\n",
    "    for iteration in range(8000):\n",
    "        # play\n",
    "        agent.eval()\n",
    "        action = agent.sample_action(observation)\n",
    "        new_observation, reward, done, info = env.step(action)\n",
    "        sars = (observation, action, reward - done, new_observation)\n",
    "        replay_buffer.add(sars)\n",
    "        if done:\n",
    "            new_observation, _ = env.reset(return_info=True)\n",
    "        observation = new_observation\n",
    "        \n",
    "        # train\n",
    "        agent.train()\n",
    "        batch = replay_buffer.sample_batch()\n",
    "        loss = trainer.train(batch)\n",
    "        losses.append(loss)\n",
    "    mean_loss = np.mean(losses, axis=0)\n",
    "    return mean_loss\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dda28875-df9e-4b44-8f0b-3b24875c68f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    agent.eval()\n",
    "    game_rewards = []\n",
    "    for i in range(20):\n",
    "        sum_reward = 0\n",
    "        observation, _ = env.reset(return_info=True)\n",
    "        for _ in range(200):\n",
    "            # action = env.action_space.sample()\n",
    "            action = agent.get_best_action(observation)\n",
    "            new_observation, reward, done, info = env.step(action)\n",
    "            sum_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        game_rewards.append(sum_reward)\n",
    "    return np.mean(game_rewards, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69d12f51-5185-4ad9-8038-459f4cdfeb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQNAgent(env.observation_space, env.action_space)\n",
    "trainer = DQNTrainer(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "570c6cca-56fb-41b0-823e-055faa23f8d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d188002-5bf0-4150-964c-1c74be9d1751",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vs/kqp818ps4gq6gdwv48lzkbk40000gn/T/ipykernel_37950/3683908536.py:12: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state_q = self.dqn_agent.backbone(torch.tensor(states))\n",
      "/var/folders/vs/kqp818ps4gq6gdwv48lzkbk40000gn/T/ipykernel_37950/3683908536.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  new_state_q = self.dqn_agent.backbone(torch.tensor(new_states))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test cumulative reward: 9.2\n",
      "Train loss: 1.007729270152436\n",
      "Train loss: 1.1049978982669335\n",
      "Train loss: 1.2404377462185696\n",
      "Train loss: 1.417643605745585\n",
      "Train loss: 1.6178649829642149\n",
      "Train loss: 1.8349394541539923\n",
      "Train loss: 2.058304997963266\n",
      "Train loss: 2.3013122145539664\n",
      "Train loss: 2.515867197002885\n",
      "Train loss: 2.7341497048270393\n",
      "Test cumulative reward: 9.55\n",
      "Train loss: 2.95156842238977\n",
      "Train loss: 3.145705130821252\n",
      "Train loss: 3.3251821132933745\n",
      "Train loss: 3.4804464753773807\n",
      "Train loss: 3.6582658036661493\n",
      "Train loss: 3.778250332602997\n",
      "Train loss: 3.937458149895746\n",
      "Train loss: 4.001721282965039\n",
      "Train loss: 4.09192989608794\n",
      "Train loss: 4.14239419361984\n",
      "Test cumulative reward: 9.2\n",
      "Train loss: 4.252531904696947\n",
      "Train loss: 4.284044665841863\n",
      "Train loss: 4.3237428002180645\n",
      "Train loss: 4.345989202062832\n",
      "Train loss: 4.42457718966566\n",
      "Train loss: 4.381616716136221\n",
      "Train loss: 4.4390048354587215\n",
      "Train loss: 4.381297177834338\n",
      "Train loss: 4.432477213003804\n",
      "Train loss: 4.419156805879639\n",
      "Test cumulative reward: 9.15\n",
      "Train loss: 4.403687088383199\n",
      "Train loss: 4.40728304663504\n",
      "Train loss: 4.364244111534339\n",
      "Train loss: 4.3249883592322265\n",
      "Train loss: 4.295783010136342\n",
      "Train loss: 4.289139638783724\n",
      "Train loss: 4.254750653249357\n",
      "Train loss: 4.162166786544753\n",
      "Train loss: 4.154855315071616\n",
      "Train loss: 4.1462036440647045\n",
      "Test cumulative reward: 9.15\n",
      "Train loss: 4.125755478073608\n",
      "Train loss: 4.107681239047604\n",
      "Train loss: 4.047993078006248\n",
      "Train loss: 3.9996030649336616\n",
      "Train loss: 3.9532444984718986\n",
      "Train loss: 3.908608330585516\n",
      "Train loss: 3.8499418394239835\n",
      "Train loss: 3.839408371003121\n",
      "Train loss: 3.7681090755120157\n",
      "Train loss: 3.7237764769806883\n",
      "Test cumulative reward: 8.9\n",
      "Train loss: 3.6535060768063135\n",
      "Train loss: 3.617418788180837\n",
      "Train loss: 3.5452941061054326\n",
      "Train loss: 3.4653125588361426\n",
      "Train loss: 3.4490716721771504\n",
      "Train loss: 3.3821321352794893\n",
      "Train loss: 3.315028112827642\n",
      "Train loss: 3.23482295308327\n",
      "Train loss: 3.161732154757911\n",
      "Train loss: 3.135179709783284\n",
      "Test cumulative reward: 9.25\n",
      "Train loss: 3.073551115682893\n",
      "Train loss: 3.024021815189906\n",
      "Train loss: 2.9485474884554996\n",
      "Train loss: 2.887831447168959\n",
      "Train loss: 2.854690212316783\n",
      "Train loss: 2.7728874614598285\n",
      "Train loss: 2.730257631022564\n",
      "Train loss: 2.6267011592792286\n",
      "Train loss: 2.583010176861556\n",
      "Train loss: 2.535185674512899\n",
      "Test cumulative reward: 9.3\n",
      "Train loss: 2.4985928340361707\n",
      "Train loss: 2.429248101153454\n",
      "Train loss: 2.360101443825413\n",
      "Train loss: 2.2931737323469803\n",
      "Train loss: 2.2464837444209236\n",
      "Train loss: 2.1917176264054614\n",
      "Train loss: 2.1303631167603116\n",
      "Train loss: 2.070125769312857\n",
      "Train loss: 2.016204112488911\n",
      "Train loss: 1.9407453613408796\n",
      "Test cumulative reward: 9.45\n",
      "Train loss: 1.890549501451665\n",
      "Train loss: 1.8315546645798857\n",
      "Train loss: 1.825385578349218\n",
      "Train loss: 1.7447555641705796\n",
      "Train loss: 1.7295943062906014\n",
      "Train loss: 1.6769400403515624\n",
      "Train loss: 1.5900061968123818\n",
      "Train loss: 1.5921527340731827\n",
      "Train loss: 1.4864378407882872\n",
      "Train loss: 1.488283078116451\n",
      "Test cumulative reward: 9.4\n",
      "Train loss: 1.4452010420120673\n",
      "Train loss: 1.3953006637425251\n",
      "Train loss: 1.4104953305681636\n",
      "Train loss: 1.3058087188192278\n",
      "Train loss: 1.266525261718244\n",
      "Train loss: 1.201097563354709\n",
      "Train loss: 1.2605917191575515\n",
      "Train loss: 1.196533754320545\n",
      "Train loss: 1.1447231424850066\n",
      "Train loss: 1.0836315780123422\n",
      "Test cumulative reward: 9.7\n",
      "Train loss: 0.9685108739079918\n",
      "Train loss: 0.9580051320810015\n",
      "Train loss: 1.0005665875797225\n",
      "Train loss: 1.0168252292051492\n",
      "Train loss: 0.9684867399975806\n",
      "Train loss: 0.9147190203828329\n",
      "Train loss: 0.8277872046675039\n",
      "Train loss: 0.8876042065783727\n",
      "Train loss: 0.8904207332356581\n",
      "Train loss: 0.8131424755672889\n",
      "Test cumulative reward: 9.4\n",
      "Train loss: 0.8007608236237557\n",
      "Train loss: 0.8011007131194513\n",
      "Train loss: 0.7620776341736686\n",
      "Train loss: 0.7235605420952425\n",
      "Train loss: 0.7327900418310092\n",
      "Train loss: 0.7278087267378565\n",
      "Train loss: 0.7688907308097882\n",
      "Train loss: 0.7421846018007243\n",
      "Train loss: 0.8034371608230131\n",
      "Train loss: 0.8346019966597024\n",
      "Test cumulative reward: 9.3\n",
      "Train loss: 0.851085922775402\n",
      "Train loss: 0.8703322449687899\n",
      "Train loss: 0.9187278983477272\n",
      "Train loss: 0.9601528190304491\n",
      "Train loss: 0.9506653715692379\n",
      "Train loss: 0.9278529083001179\n",
      "Train loss: 0.9939007762109108\n",
      "Train loss: 0.9807858193965957\n",
      "Train loss: 1.0135784525950295\n",
      "Train loss: 0.9284037604417503\n",
      "Test cumulative reward: 9.15\n",
      "Train loss: 0.9464288853187499\n",
      "Train loss: 1.1341699994380283\n",
      "Train loss: 1.072952533500802\n",
      "Train loss: 1.1160422282159457\n",
      "Train loss: 1.16662394827081\n",
      "Train loss: 1.1435062241563791\n",
      "Train loss: 1.257891859793507\n",
      "Train loss: 1.3427292790181866\n",
      "Train loss: 1.3824483082684824\n",
      "Train loss: 1.4893854265552995\n",
      "Test cumulative reward: 9.35\n",
      "Train loss: 1.5535577841343793\n",
      "Train loss: 1.5724236888370406\n",
      "Train loss: 1.6241213961388061\n",
      "Train loss: 1.6335647861900382\n",
      "Train loss: 1.6489535144378646\n",
      "Train loss: 1.6773735050609524\n",
      "Train loss: 1.6695470960941443\n",
      "Train loss: 1.7241809560630512\n",
      "Train loss: 1.669844185543393\n",
      "Train loss: 1.6982678408215046\n",
      "Test cumulative reward: 9.4\n",
      "Train loss: 1.7251578019805724\n",
      "Train loss: 1.7309319099930414\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "for i in range(200):\n",
    "    if (i % 10 == 0):\n",
    "        score = evaluate()\n",
    "        scores.append(score)\n",
    "        print(f\"Test cumulative reward: {score}\")\n",
    "    train_loss = train()\n",
    "    print(f\"Train loss: {train_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a4c7f5-0012-4cf3-9644-59646900908c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe131191-96a8-45ac-a4db-a75b32acc0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer.sample_batch()[2].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bd7e39-18af-4f45-aa6b-d2358628d0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer.sample_batch()[2].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5568f50d-8e3d-48e1-afe8-5a9ba8be98a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(replay_buffer._buffer.buffer[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
